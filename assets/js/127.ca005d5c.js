(window.webpackJsonp=window.webpackJsonp||[]).push([[127],{1214:function(t,s,a){"use strict";a.r(s);var n=a(18),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,n=t._self._c||s;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h1",{attrs:{id:"pandas"}},[t._v("Pandas")]),t._v(" "),n("ul",[n("li",[n("a",{attrs:{href:"https://pandas.pydata.org/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Pandas"),n("OutboundLink")],1),t._v(" 是 Python 中的数据操纵和分析软件包")]),t._v(" "),n("li",[t._v("Pandas 名称取自于**计量经济学"),n("em",[t._v("Panel Data")]),t._v("（面板数据）**一词")]),t._v(" "),n("li",[t._v("Pandas 为 Python 带来了两个新的数据结构，能够轻松直观地处理"),n("strong",[t._v("带标签数据")]),t._v("和"),n("strong",[t._v("关系数据")]),t._v(" "),n("ul",[n("li",[t._v("Pandas Series")]),t._v(" "),n("li",[t._v("Pandas DataFrame")])])])]),t._v(" "),n("p",[t._v("机器学习需要大量数据，但首先需要检查数据，Pandas 可对数据进行快速分析和操纵，主要的几个功能：")]),t._v(" "),n("ul",[n("li",[t._v("允许为行和列"),n("strong",[t._v("设定标签")]),t._v("，可为"),n("strong",[t._v("索引指定任何名称")])]),t._v(" "),n("li",[t._v("可以针对"),n("strong",[t._v("时间序列数据")]),t._v("计算滚动"),n("strong",[t._v("统计学指标")])]),t._v(" "),n("li",[t._v("轻松地"),n("strong",[t._v("处理 "),n("code",[t._v("NaN")]),t._v(" 值")])]),t._v(" "),n("li",[t._v("能够将"),n("strong",[t._v("不同格式的数据")]),t._v("加载到 DataFrame 中")]),t._v(" "),n("li",[t._v("可以将"),n("strong",[t._v("不同的数据集合")]),t._v("并到一起")]),t._v(" "),n("li",[n("strong",[t._v("与 NumPy 和 Matplotlib 集成")])])]),t._v(" "),n("p",[t._v("参考：Pandas 文档"),n("a",{attrs:{href:"https://pandas.pydata.org/pandas-docs/stable/",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://pandas.pydata.org/pandas-docs/stable/"),n("OutboundLink")],1)]),t._v(" "),n("h2",{attrs:{id:"下载"}},[t._v("下载")]),t._v(" "),n("p",[t._v("Anaconda 中包含 Pandas")]),t._v(" "),n("h3",{attrs:{id:"安装特定版本"}},[t._v("安装特定版本")]),t._v(" "),n("p",[t._v("在 Anaconda 提示符处输入 "),n("code",[t._v("conda install pandas=0.22")]),t._v(" 安装指定 Pandas 版本")]),t._v(" "),n("h3",{attrs:{id:"版本"}},[t._v("版本")]),t._v(" "),n("p",[t._v("检查你的 Pandas 版本")]),t._v(" "),n("ul",[n("li",[t._v("在 Jupyter notebook 中输入 "),n("code",[t._v("!conda list pandas")])]),t._v(" "),n("li",[t._v("在 Anaconda 提示符处输入 "),n("code",[t._v("conda list pandas")])])]),t._v(" "),n("h3",{attrs:{id:"导入"}},[t._v("导入")]),t._v(" "),n("p",[t._v("在 Jupyter Notebook 中键入以导入 Pandas，一般命名为 "),n("code",[t._v("pd")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n")])])]),n("h1",{attrs:{id:"pandas-series"}},[t._v("Pandas Series")]),t._v(" "),n("p",[t._v("Pandas Series（Pandas 序列）是 像"),n("em",[t._v("数组")]),t._v("一样的"),n("strong",[t._v("一维对象")]),t._v("，可以存储很"),n("strong",[t._v("多类型")]),t._v("的数据（相应地** Numpy "),n("strong",[t._v("数组中所有元素")]),t._v("类型需要一样**）")]),t._v(" "),n("h2",{attrs:{id:"创建series"}},[t._v("创建Series")]),t._v(" "),n("p",[t._v("先导入 "),n("code",[t._v("pandas")]),t._v(" 库，再使用函数 "),n("code",[t._v("pd.Series(data, index)")]),t._v(" 创建序列，其中 "),n("code",[t._v("index")]),t._v(" 是一个"),n("strong",[t._v("索引标签列表")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建一个购物清单，清单的元素类型不同，索引改为标签")]),t._v("\ngroceries "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Series"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Yes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'No'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'eggs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'apples'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'milk'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bread'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# We display the Groceries Pandas Series")]),t._v("\ngroceries\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("eggs           "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\napples         "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v("\nmilk         Yes\nbread       No\ndtype: object\n")])])]),n("h2",{attrs:{id:"series属性"}},[t._v("Series属性")]),t._v(" "),n("h3",{attrs:{id:"series维度"}},[t._v("Series维度")]),t._v(" "),n("p",[t._v("使用属性 "),n("code",[t._v(".ndim")]),t._v(" 返回序列的维度，一般为 "),n("code",[t._v("1")])]),t._v(" "),n("h3",{attrs:{id:"series形状"}},[t._v("Series形状")]),t._v(" "),n("ul",[n("li",[t._v("序列的形状是指每个维度的大小，因为序列是一维，因此的返回的是元素的"),n("strong",[t._v("数量")])]),t._v(" "),n("li",[t._v("使用属性 "),n("code",[t._v(".shape")]),t._v(" 返回序列的维度大小")])]),t._v(" "),n("h3",{attrs:{id:"series大小"}},[t._v("Series大小")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".size")]),t._v(" 返回序列的元素总数")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# We print some information about Groceries")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Groceries has shape:'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" groceries"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Groceries has dimension:'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" groceries"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ndim"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Groceries has a total of'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" groceries"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'elements'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("Groceries has shape: "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v(","),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nGroceries has dimension: "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\nGroceries has a total of "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v(" elements\n")])])]),n("h3",{attrs:{id:"输出索引标签"}},[t._v("输出索引标签")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".index")]),t._v(" 单独输出 Series 索引标签")]),t._v(" "),n("h3",{attrs:{id:"输出数据"}},[t._v("输出数据")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".values")]),t._v(" 输出 Series 元素的值")]),t._v(" "),n("h2",{attrs:{id:"查询索引标签"}},[t._v("查询索引标签")]),t._v(" "),n("p",[t._v("使用关键字 "),n("code",[t._v("in")]),t._v(" 查询是否存在某个索引标签，返回布尔值")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n\ngroceries "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Series"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Yes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'No'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'eggs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'apples'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'milk'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bread'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nx "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bananas'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" groceries\ny "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bread'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" groceries\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Is bananas an index label in Groceries:'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Is bread an index label in Groceries:'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("Is bananas an index label "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" Groceries: False\nIs bread an index label "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" Groceries: True\n")])])]),n("h2",{attrs:{id:"编辑series"}},[t._v("编辑Series")]),t._v(" "),n("h3",{attrs:{id:"访问元素"}},[t._v("访问元素")]),t._v(" "),n("h4",{attrs:{id:"方括号添加数字索引"}},[t._v("方括号添加数字索引")]),t._v(" "),n("ul",[n("li",[t._v("方括号 "),n("code",[t._v("[]")]),t._v(" 内添加"),n("strong",[t._v("数字索引")]),t._v("访问元素，正整数从开头访问数据，使用负整数从末尾访问")]),t._v(" "),n("li",[t._v("为了区别使用索引类型，使用属性 "),n("code",[t._v(".iloc")]),t._v("（即 integer location）用于"),n("strong",[t._v("明确表明使用的是数字索引")])]),t._v(" "),n("li",[t._v("可输入"),n("strong",[t._v("数字列表")]),t._v("以访问多个相应的元素")])]),t._v(" "),n("h4",{attrs:{id:"方括号添加索引标签"}},[t._v("方括号添加索引标签")]),t._v(" "),n("ul",[n("li",[t._v("方括号 "),n("code",[t._v("[ ]")]),t._v(" 内添加"),n("strong",[t._v("索引标签")]),t._v("访问元素")]),t._v(" "),n("li",[t._v("为了区别使用索引类型，使用属性 "),n("code",[t._v(".loc")]),t._v("（即 location）用于"),n("strong",[t._v("明确表明使用的是标签索引")])]),t._v(" "),n("li",[t._v("可输入"),n("strong",[t._v("索引标签列表")]),t._v("以访问多个相应的元素")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\ngroceries "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Series"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Yes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'No'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'eggs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'apples'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'milk'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bread'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用索引标签访问元素")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'How many eggs do we need to buy:'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" groceries"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'eggs'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 输入索引标签列表访问多个元素")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Do we need milk and bread:\\n'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" groceries"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'milk'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bread'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#使用数字索引访问元素")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'How many eggs and apples do we need to buy:\\n'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  groceries"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("How many eggs "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("do")]),t._v(" we need to buy: "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\n\nDo we need milk and bread:\nmilk       Yes\nbread     No\ndtype: object\n\nHow many eggs and apples "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("do")]),t._v(" we need to buy:\neggs       "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\napples     "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v("\ndtype: object\n")])])]),n("h3",{attrs:{id:"修改元素"}},[t._v("修改元素")]),t._v(" "),n("p",[t._v("创建好 Pandas Series 后可更改其元素，使用"),n("strong",[t._v("赋值符号 "),n("code",[t._v("=")])]),t._v(" 可以将新的值赋给相应的元素")]),t._v(" "),n("h3",{attrs:{id:"删除元素"}},[t._v("删除元素")]),t._v(" "),n("ul",[n("li",[t._v("使用方法 "),n("code",[t._v(".drop(label)")]),t._v(" 删除 Pandas Series 中的条目（元素），但该方法"),n("strong",[t._v("不会更改原始 Series")]),t._v("，而是返回一个"),n("strong",[t._v("修改后的副本")])]),t._v(" "),n("li",[t._v("使用方法 "),n("code",[t._v(".drop(label, inplace=True)")]),t._v(" 并将关键字 "),n("code",[t._v("inplace")]),t._v(" 替代为 "),n("code",[t._v("True")]),t._v(" 可以原地从 Pandas Series 中删除条目")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Original Grocery List:\\n'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" groceries"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'We remove apples (out of place):\\n'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" groceries"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'apples'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Grocery List after removing apples out of place:\\n'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" groceries"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ngroceries"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'apples'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inplace "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Grocery List after removing apples in place:\\n'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" groceries"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("Original Grocery List:\neggs           "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\napples         "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v("\nmilk         Yes\nbread       No\ndtype: object\n\nWe remove apples "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("out of place"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(":\neggs           "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\nmilk         Yes\nbread       No\ndtype: object\n\nGrocery List after removing apples out of place:\neggs           "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\napples         "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v("\nmilk         Yes\nbread       No\ndtype: object\n\nGrocery List after removing apples "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" place:\neggs           "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\nmilk         Yes\nbread       No\ndtype: object\n")])])]),n("h2",{attrs:{id:"运算"}},[t._v("运算")]),t._v(" "),n("h3",{attrs:{id:"算术运算"}},[t._v("算术运算")]),t._v(" "),n("p",[t._v("可以对 Series 执行"),n("strong",[t._v("所有")]),t._v("或"),n("strong",[t._v("特定")]),t._v("元素级算术运算，还可以导入 NumPy 使用数学函数")]),t._v(" "),n("p",[t._v("⚠️ 由于 Pandas 元素的数据类型可并不统一，因此进行运算是需要确保算术运算对于所操作的"),n("strong",[t._v("元素数据类型有效")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("fruits"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Series"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'apples'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'oranges'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bananas'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Original grocery list of fruits:\\n '")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fruits"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对序列 fruits 所有元素进行算术运算")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'fruits + 2:\\n'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fruits "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# We add 2 to each item in fruits")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对序列 fruit 中具体的条目（元素）进行运算")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Amount of bananas + 2 = '")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fruits"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bananas'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 导入 NumPy 模块，使用内建函数")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'SQRT(X) =\\n'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sqrt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fruits"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("Original grocery list of fruits:\napples         "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\noranges        "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v("\nbananas       "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\ndtype: int64\n\nfruits + "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v(":\napples         "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),t._v("\noranges        "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),t._v("\nbananas       "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\ndtype: int64\n\nSQRT"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("X"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("\napples            "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.162278")]),t._v("\noranges         "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.449490")]),t._v("\nbananas        "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.732051")]),t._v("\ndtype: float64\n")])])]),n("h1",{attrs:{id:"pandas-dataframe"}},[t._v("Pandas DataFrame")]),t._v(" "),n("p",[t._v("Pandas DataFrames 是可带"),n("strong",[t._v("标签的行和列")]),t._v("的"),n("strong",[t._v("二维数据结构")]),t._v("，可存储"),n("strong",[t._v("多类型的数据")]),t._v("，类似于电子表格")]),t._v(" "),n("h2",{attrs:{id:"创建dataframe"}},[t._v("创建DataFrame")]),t._v(" "),n("p",[t._v("使用字典创建 DataFrame")]),t._v(" "),n("ul",[n("li",[t._v("使用函数 "),n("code",[t._v("pd.DataFrame(dict)")]),t._v(" 手动创建 DataFrame")]),t._v(" "),n("li",[t._v("传递给函数的是"),n("strong",[t._v("字典")]),t._v(" "),n("ul",[n("li",[t._v("字典的元素的"),n("strong",[t._v("键 "),n("code",[t._v("key")])]),t._v(" 是 DataFrame 的"),n("strong",[t._v("列标签")])]),t._v(" "),n("li",[t._v("字典的元素"),n("strong",[t._v("值 "),n("code",[t._v("value")])]),t._v(" 是 DataFrame 的数据")])])]),t._v(" "),n("li",[t._v("DataFrame 的"),n("strong",[t._v("行标签")]),t._v("可使用"),n("strong",[t._v("关键字 "),n("code",[t._v("index")]),t._v(" 添加")]),t._v("，默认使用数字索引，从 "),n("code",[t._v("0")]),t._v(" 开始标记行")])]),t._v(" "),n("h3",{attrs:{id:"使用列表-数组-字典"}},[t._v("使用列表（数组）字典")]),t._v(" "),n("p",[t._v("创建一个字典传递给 "),n("code",[t._v("pd.DataFrame()")]),t._v(" 函数，字典中的"),n("strong",[t._v("所有列表（数组）长度必须一样")]),t._v("。")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("data "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Integers'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Floats'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8.2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("9.6")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\ndf1 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"df1 is:\\n"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" df1"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建 DataFrame 并添加标签")]),t._v("\ndf2 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'label 1'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'label 2'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'label 3'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"df2 is:\\n"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" df2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("df1 is:\n        Floats\tIntegers\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8.2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("9.6")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\n\ndf2 is:\n\tFloats\tIntegers\nlabel "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\nlabel "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8.2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\nlabel "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("9.6")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\n")])])]),n("h3",{attrs:{id:"使用-pandas-series-字典"}},[t._v("使用 Pandas Series 字典")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 创建一个字典，其中 value 是 Pandsa Series 结构，所包含的 index 作为行标签")]),t._v("\nitems "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Bob'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Series"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("245")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("25")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("55")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bike'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'watch'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n         "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Alice'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Series"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("40")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("110")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("500")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'book'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'glasses'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bike'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将字典传递给函数 pd.DataFrame() 其中 key 作为列标签")]),t._v("\nshopping_carts "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nshopping_carts\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("\tAlice\tBob\nbike\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("500.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("245.0")]),t._v("\nbook\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("40.0")]),t._v("\tNaN\nglasses\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("110.0")]),t._v("\tNaN\npants\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("25.0")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("watch")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("55.0")]),t._v("\n")])])]),n("p",[t._v("注意：")]),t._v(" "),n("ul",[n("li",[t._v("DataFrame 以表格形式显示，行和列的标签以"),n("strong",[t._v("粗体形式显示")])]),t._v(" "),n("li",[t._v("DataFrame 的"),n("strong",[t._v("行标签")]),t._v("根据构建字典所用的两个 "),n("strong",[t._v("Pandas Series 的索引标签")]),t._v("创建而成")]),t._v(" "),n("li",[t._v("DataFrame 的列标签来自字典的键")]),t._v(" "),n("li",[t._v("列"),n("strong",[t._v("按字母顺序排序")]),t._v("，而不是字典中的顺序（但从数据文件中向 DataFrame 加载数据时，则按照数据原有顺序排列）")]),t._v(" "),n("li",[n("code",[t._v("NaN")]),t._v(" 是指非数字（Not a Number），Pandas 用 "),n("code",[t._v("NaN")]),t._v(" 值填充行或列索引对应的位置中，没有值的元素")])]),t._v(" "),n("h3",{attrs:{id:"使用-python-字典列表"}},[t._v("使用 Python 字典列表")]),t._v(" "),n("p",[t._v("使用由"),n("strong",[t._v("字典构成元素")]),t._v("的列表，传递给函数 "),n("code",[t._v("pd.DataFrame()")]),t._v(" 创建 DataFrame")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("items2 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bikes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'watches'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'watches'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'glasses'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bikes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\nstore_items "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("items2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'store 1'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'store 2'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nstore_items\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("\tbikes\tglasses\tpants\twatches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\n")])])]),n("h3",{attrs:{id:"合并产生新dataframe"}},[t._v("合并产生新DataFrame")]),t._v(" "),n("h4",{attrs:{id:"合并类型"}},[t._v("合并类型")]),t._v(" "),n("p",[t._v("Pandas 中有四种合并类型：")]),t._v(" "),n("p",[n("img",{attrs:{src:a(607),alt:"Pandas 中的四种合并类型"}})]),t._v(" "),n("ul",[n("li",[t._v("内联 "),n("code",[t._v("inner join")]),t._v(" 使用两个 dataframe 中的键的"),n("strong",[t._v("交集")])]),t._v(" "),n("li",[t._v("外联 "),n("code",[t._v("outer join")]),t._v(" 使用两个 dataframe 的键的"),n("strong",[t._v("并集")])]),t._v(" "),n("li",[t._v("左联 "),n("code",[t._v("left join")]),t._v(" 仅使用来自"),n("strong",[t._v("左 dataframe 的键")])]),t._v(" "),n("li",[t._v("右联 "),n("code",[t._v("right join")]),t._v(" 仅使用来自"),n("strong",[t._v("右 dataframe的键")])])]),t._v(" "),n("p",[n("strong",[t._v("键")]),t._v("指我们将进行连接的两个 dataframe 中的"),n("strong",[t._v("共同列")])]),t._v(" "),n("h4",{attrs:{id:"函数merge"}},[t._v("函数merge")]),t._v(" "),n("p",[t._v("使用"),n("a",{attrs:{href:"https://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging",target:"_blank",rel:"noopener noreferrer"}},[t._v("函数"),n("OutboundLink")],1),t._v("实现根据一个或多个"),n("strong",[t._v("键")]),t._v(" "),n("code",[t._v("key")]),t._v(" 将数据帧的合并 "),n("code",[t._v("pd.merge(left, right, how='inner')")])]),t._v(" "),n("ul",[n("li",[t._v("默认采用 "),n("code",[t._v("how='inner'")]),t._v(" 的合并形式，可通过修改该参数实现 "),n("code",[t._v("left")]),t._v("、"),n("code",[t._v("right")]),t._v("、"),n("code",[t._v("outer")]),t._v("、"),n("code",[t._v("inner")]),t._v(" 四种形式")]),t._v(" "),n("li",[t._v("可使用参数 "),n("code",[t._v("left_on")]),t._v("、"),n("code",[t._v("right_on")]),t._v(" 设置合并时以"),n("strong",[t._v("何（几）列")]),t._v("作为相匹配的"),n("strong",[t._v("键")]),t._v("（即以这些列的值是否匹配，（再根据合并的形式 "),n("code",[t._v("how")]),t._v("）来决定是否合并对应的行）")])]),t._v(" "),n("p",[t._v("也可以作为方法使用 "),n("code",[t._v("df1.merge(df2)")]),t._v(" 将 "),n("code",[t._v("df1")]),t._v(" 与 "),n("code",[t._v("df2")]),t._v(" 合并，"),n("strong",[t._v("默认")]),t._v("将 "),n("code",[t._v("right=df2")]),t._v(" 合并到 "),n("code",[t._v("left=df1")]),t._v(" 右侧，使用方式是 "),n("code",[t._v("inner")])]),t._v(" "),n("p",[t._v("⚠️ 区分"),n("strong",[t._v("附加")]),t._v("方法 "),n("code",[t._v("append()")]),t._v(" 竖向合并/添加数据")]),t._v(" "),n("h4",{attrs:{id:"方法join"}},[t._v("方法join")]),t._v(" "),n("p",[t._v("连接其他数据帧的列，可选择基于标签 "),n("code",[t._v("index")]),t._v(" 或关键列")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" caller\n    A key\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("  A0  K0\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("  A1  K1\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("  A2  K2\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("  A3  K3\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("  A4  K4\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("  A5  K5\n\n"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" other\n    B key\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("  B0  K0\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("  B1  K1\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("  B2  K2\n\n"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" caller"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("other"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lsuffix"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'_caller'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" rsuffix"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'_other'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 添加后缀，为数据帧中名称重复的列添加后缀，以区别开来， lsuffix 左侧数据帧，rsuffix 右侧数据帧")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("     A key_caller    B key_other\n    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("  A0         K0   B0        K0\n    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("  A1         K1   B1        K1\n    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("  A2         K2   B2        K2\n    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("  A3         K3  NaN       NaN\n    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("  A4         K4  NaN       NaN\n    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("  A5         K5  NaN       NaN\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 按照关键列 key 进行合并，并以 key 列作为行标签")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" caller"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("other"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("set_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'key'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" on"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'key'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v("     A key    B\n    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("  A0  K0   B0\n    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("  A1  K1   B1\n    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("  A2  K2   B2\n    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("  A3  K3  NaN\n    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("  A4  K4  NaN\n    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("  A5  K5  NaN\n")])])]),n("h3",{attrs:{id:"加载数据到dataframe"}},[t._v("加载数据到DataFrame")]),t._v(" "),n("p",[t._v("Pandas 使用不同的函数将不同格式的数据库加载到 DataFrame 中")]),t._v(" "),n("p",[t._v("加载"),n("strong",[t._v("结构性")]),t._v("文件（使用特定分隔符分隔的数据），可以结合关键字 "),n("code",[t._v("sep")]),t._v(" 使用，读取数据")]),t._v(" "),n("h4",{attrs:{id:"加载csv文件"}},[t._v("加载csv文件")]),t._v(" "),n("ul",[n("li",[t._v("CSV 是指"),n("strong",[t._v("使用逗号 "),n("code",[t._v(",")]),t._v(" 分隔值的数据文件")]),t._v("，是一种简单的数据存储格式")]),t._v(" "),n("li",[t._v("使用 "),n("code",[t._v("pd.read_csv(file)")]),t._v(" "),n("a",{attrs:{href:"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("函数"),n("OutboundLink")],1),t._v("将 CSV 文件加载到 Pandas DataFrame 中，默认关键字 "),n("code",[t._v("sep = ','")]),t._v("，修改此参数可以使用该函数读取"),n("strong",[t._v("使用其他分隔符")]),t._v("存储的数据文件。")]),t._v(" "),n("li",[t._v("使用关键字 "),n("code",[t._v("header = row_number")]),t._v(" 指定文件的哪一行作为"),n("strong",[t._v("标题")]),t._v("（即指定了"),n("strong",[t._v("列标签")]),t._v("），通常数据文件第一行作为标题。\n"),n("ul",[n("li",[t._v("如果"),n("strong",[t._v("文件顶部有额外的元信息")]),t._v("，则需要指定另外的行作为标题行")]),t._v(" "),n("li",[t._v("如果数据文件中"),n("strong",[t._v("不包括列标签")]),t._v("，可以使用 "),n("code",[t._v("header=None")]),t._v(" 取消将数据文件第一行作为列标签")]),t._v(" "),n("li",[t._v("可以使用关键字 "),n("code",[t._v("names")]),t._v(" "),n("strong",[t._v("自定义标题（列标签）")]),t._v("，通过 "),n("code",[t._v("names=labels")]),t._v(" 手动添加标签，其数量应该与数据集的 "),n("code",[t._v("columns")]),t._v(" 数量相同")])])]),t._v(" "),n("li",[t._v("索引除了可使用默认索引（从 0 递增 1 的整数）之外，还可以将一个或多个"),n("strong",[t._v("列指定为数据框的（行）索引")]),t._v("，使用关键字 "),n("code",[t._v("index_col='Name'")]),t._v(" 指定列作为行索引")])]),t._v(" "),n("h2",{attrs:{id:"查看dataframe"}},[t._v("查看DataFrame")]),t._v(" "),n("h3",{attrs:{id:"查看数据集部分数据"}},[t._v("查看数据集部分数据")]),t._v(" "),n("ul",[n("li",[t._v("使用方法 "),n("code",[t._v(".head(N)")]),t._v(" 查看数据集前 "),n("code",[t._v("N")]),t._v(" 行的数据，默认查看前5行数据")]),t._v(" "),n("li",[t._v("使用方法 "),n("code",[t._v(".tail(N)")]),t._v(" 查看数据集最后 "),n("code",[t._v("N")]),t._v(" 行的数据，默认查看最后5行数据")])]),t._v(" "),n("h3",{attrs:{id:"查看数据集nan情况"}},[t._v("查看数据集NaN情况")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".isnull")]),t._v(" 和 "),n("code",[t._v(".any()")]),t._v(" 检查"),n("strong",[t._v("任何列")]),t._v("（默认参数 "),n("code",[t._v("axis=0")]),t._v("）是否包含 "),n("code",[t._v("NaN")]),t._v(" 值，可修改参数 "),n("code",[t._v("axis=1")]),t._v(" 检查"),n("strong",[t._v("任何行")]),t._v("是否包含 "),n("code",[t._v("NaN")]),t._v(" 值")]),t._v(" "),n("p",[t._v("结合方法 "),n("code",[t._v(".sum()")]),t._v(" 统计"),n("strong",[t._v("列数（或行数）")])]),t._v(" "),n("h3",{attrs:{id:"查看数据集统计信息"}},[t._v("查看数据集统计信息")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".describe()")]),t._v(" 返回"),n("strong",[t._v("每列")]),t._v("的描述性统计信息：")]),t._v(" "),n("ul",[n("li",[n("code",[t._v("count")]),t._v(" 统计每列的行数")]),t._v(" "),n("li",[n("code",[t._v("mean")]),t._v(" 计算每列的平均值")]),t._v(" "),n("li",[n("code",[t._v("std")])]),t._v(" "),n("li",[n("code",[t._v("min")])]),t._v(" "),n("li",[n("code",[t._v("25%")])]),t._v(" "),n("li",[n("code",[t._v("50%")])]),t._v(" "),n("li",[n("code",[t._v("75%")])]),t._v(" "),n("li",[n("code",[t._v("max")])])]),t._v(" "),n("p",[t._v("可以给方法传递"),n("strong",[t._v("列名")]),t._v("返回"),n("strong",[t._v("特定列的描述性统计信息")])]),t._v(" "),n("h3",{attrs:{id:"查看唯一值"}},[t._v("查看唯一值")]),t._v(" "),n("ul",[n("li",[n("p",[t._v("使用方法 "),n("code",[t._v(".describe()")]),t._v(" 查询"),n("strong",[t._v("类型为 "),n("code",[t._v("object")]),t._v("（即字符串）列")]),t._v("的唯一值信息")])]),t._v(" "),n("li",[n("p",[t._v("使用函数 "),n("code",[t._v("np.unique(df[['column_name']])")]),t._v(" 返回 dataframe 特定列 "),n("code",[t._v("column_name")]),t._v(" 中的"),n("strong",[t._v("非空唯一值")])]),t._v(" "),n("p",[t._v("⚠️ 访问 "),n("code",[t._v("column_name")]),t._v(" 需要使用"),n("strong",[t._v("双方括号 "),n("code",[t._v("[[]]")])]),t._v(" 包括，返回一个** Series 类型**")])]),t._v(" "),n("li",[n("p",[t._v("使用函数 "),n("code",[t._v("pd.value_counts(df)")]),t._v(" 返回一个对唯一值统计的 list")]),t._v(" "),n("p",[t._v("使用 "),n("code",[t._v("len(pd.value_counts(df['column_name']).index)")]),t._v(" 返回特定列的唯一值个数")])])]),t._v(" "),n("h3",{attrs:{id:"相关性"}},[t._v("相关性")]),t._v(" "),n("p",[t._v("使用 "),n("code",[t._v(".corr()")]),t._v(" 方法获取不同列之间的关联性")]),t._v(" "),n("h3",{attrs:{id:"分组数据"}},[t._v("分组数据")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".groupby()")]),t._v(" 能够以不同的方式"),n("strong",[t._v("对数据分组")])]),t._v(" "),n("p",[t._v("对于没有明确分组的变量（如连续变量），可以使用"),n("a",{attrs:{href:"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("函数"),n("OutboundLink")],1),t._v(" "),n("code",[t._v("pd.cut(x, bins)")]),t._v("** 对数据进行自定义（按照 "),n("code",[t._v("bin")]),t._v(" 作为标准）切分**，默认划分为"),n("strong",[t._v("左开右闭区间")])]),t._v(" "),n("ul",[n("li",[t._v("输入的 "),n("code",[t._v("x")]),t._v(" 是"),n("strong",[t._v("一维")]),t._v("的 "),n("code",[t._v("array")]),t._v(" 对象")]),t._v(" "),n("li",[t._v("参数 "),n("code",[t._v("bins")]),t._v(" 设置分组方法\n"),n("ul",[n("li",[t._v("当 "),n("code",[t._v("bins=n")]),t._v("（"),n("code",[t._v("n")]),t._v(" 为正整数），则将数据 "),n("code",[t._v("x")]),t._v(" 分成等大小的 "),n("code",[t._v("n")]),t._v(" 份，返回一个区间 Serise（记录 "),n("code",[t._v("x")]),t._v(" 中各元素所在的区间）")]),t._v(" "),n("li",[t._v("若 "),n("code",[t._v("bins=list")]),t._v("（"),n("code",[t._v("list")]),t._v(" 为列表，其中元素"),n("strong",[t._v("从小到大依次排列")]),t._v("），则将数据按照列表元素划分区间，返回一个区间 Series（记录 "),n("code",[t._v("x")]),t._v(" 中各元素所在的区间）")])])]),t._v(" "),n("li",[t._v("可使用 "),n("code",[t._v("index=list_name")]),t._v(" 为划分的区间"),n("strong",[t._v("命名")]),t._v("，注意 "),n("code",[t._v("list_name")]),t._v(" 标签数量和划分区间数量相同")])]),t._v(" "),n("p",[t._v("具体方法参阅："),n("a",{attrs:{href:"https://medium.com/@morris_tai/pandas%E7%9A%84cut-qcut%E5%87%BD%E6%95%B8-93c244e34cfc",target:"_blank",rel:"noopener noreferrer"}},[t._v("pandas的cut&qcut 函數"),n("OutboundLink")],1)]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("data "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./fake_company.csv'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndata\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 按照年份分组（行标签），分析薪资的总和（列标签）")]),t._v("\ndata"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groupby"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Year'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Salary'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 依次按照年份和部门分组（行标签），分析薪资的总和（列标签）")]),t._v("\ndata"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groupby"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Year'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Department'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Salary'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("        Year\tName\tDepartment\tAge\tSalary\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1990")]),t._v("\tAlice\tHR\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("25")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50000")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1990")]),t._v("\tBob\tRD\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("48000")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1990")]),t._v("\tCharlie\tAdmin\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("55000")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1991")]),t._v("\tAlice\tHR\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("26")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("52000")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1991")]),t._v("\tBob\tRD\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("31")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50000")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1991")]),t._v("\tCharlie\tAdmin\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("46")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("60000")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1992")]),t._v("\tAlice\tAdmin\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("27")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("60000")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1992")]),t._v("\tBob\tRD\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("52000")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1992")]),t._v("\tCharlie\tAdmin\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("62000")]),t._v("\n\nYear\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1990")]),t._v("     "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("153000")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1991")]),t._v("     "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("162000")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1992")]),t._v("     "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("174000")]),t._v("\nName: Salary, dtype: int64\n\nYear     Department\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1990")]),t._v("    Admin              "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("55000")]),t._v("\n             HR                    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50000")]),t._v("\n             RD                    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("48000")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1991")]),t._v("    Admin              "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("60000")]),t._v("\n             HR                    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("52000")]),t._v("\n             RD                    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50000")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1992")]),t._v("    Admin            "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("122000")]),t._v("\n             RD                    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("52000")]),t._v("\nName: Salary, dtype: int64\n")])])]),n("h2",{attrs:{id:"dataframe属性"}},[t._v("DataFrame属性")]),t._v(" "),n("p",[t._v("使用属性从 DataFrame 中提取信息")]),t._v(" "),n("h3",{attrs:{id:"dataframe元素数据类型"}},[t._v("DataFrame元素数据类型")]),t._v(" "),n("p",[t._v("使用属性 "),n("code",[t._v(".dtypes")]),t._v(" 查看"),n("strong",[t._v("每列元素的类型")])]),t._v(" "),n("p",[t._v("Pandas 实际上将 dataframe 和序列中的"),n("strong",[t._v("字符串存储为"),n("a",{attrs:{href:"https://en.wikipedia.org/wiki/Pointer_(computer_programming)",target:"_blank",rel:"noopener noreferrer"}},[t._v("指针"),n("OutboundLink")],1)]),t._v("，因此，数据类型是 "),n("code",[t._v("object")]),t._v(" 而不是 "),n("code",[t._v("str")])]),t._v(" "),n("h3",{attrs:{id:"dataframe维度"}},[t._v("DataFrame维度")]),t._v(" "),n("p",[t._v("使用属性 "),n("code",[t._v(".ndim")]),t._v(" 返回 DataFrame 维度，一般为 "),n("code",[t._v("2")])]),t._v(" "),n("h3",{attrs:{id:"dataframe形状"}},[t._v("DataFrame形状")]),t._v(" "),n("ul",[n("li",[t._v("数据框的形状是指每个维度的大小，因为序列是二维，因此的返回的分别是"),n("strong",[t._v("行和列的数量")])]),t._v(" "),n("li",[t._v("使用属性 "),n("code",[t._v(".shape")]),t._v(" 返回 DataFrame 各维度大小")])]),t._v(" "),n("h3",{attrs:{id:"dataframe大小"}},[t._v("DataFrame大小")]),t._v(" "),n("p",[t._v("使用属性 "),n("code",[t._v(".size")]),t._v(" 返回 DataFrame 大小，即元素个数")]),t._v(" "),n("h3",{attrs:{id:"输出数据-2"}},[t._v("输出数据")]),t._v(" "),n("p",[t._v("使用属性 "),n("code",[t._v(".values")]),t._v(" 返回 DataFrame 数据部分")]),t._v(" "),n("h3",{attrs:{id:"输出行标签"}},[t._v("输出行标签")]),t._v(" "),n("p",[t._v("使用属性 "),n("code",[t._v(".index")]),t._v(" 返回 DataFrame 行标签")]),t._v(" "),n("h3",{attrs:{id:"输出列标签"}},[t._v("输出列标签")]),t._v(" "),n("p",[t._v("使用属性 "),n("code",[t._v(".columns")]),t._v(" 返回 DataFrame 列标签")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("data "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Bob'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Series"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("245")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("25")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("55")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Alice'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Series"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("40")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("110")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("500")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\ndf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shopping_carts has shape:'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shopping_carts"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shopping_carts has dimension:'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shopping_carts"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ndim"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shopping_carts has a total of:'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shopping_carts"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'elements'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'The data in shopping_carts is:\\n'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shopping_carts"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'The row index in shopping_carts is:'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shopping_carts"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'The column index in shopping_carts is:'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" shopping_carts"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("columns"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("shopping_carts has shape: "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v(", "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nshopping_carts has dimension: "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\nshopping_carts has a total of: "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v(" elements\n\nThe data "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" shopping_carts is:\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("500")]),t._v(".    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("245")]),t._v("."),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("       "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("40")]),t._v(".     nan"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("     "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("110")]),t._v(".     nan"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("       "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45")]),t._v(".      "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("25")]),t._v("."),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("     nan       "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("55")]),t._v("."),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\nThe row index "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" shopping_carts is: Index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bike'")]),t._v(", "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'book'")]),t._v(", "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'glasses'")]),t._v(", "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),t._v(", "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'watch'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(", "),n("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("dtype")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'object'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nThe "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("column")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" shopping_carts is: Index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Alice'")]),t._v(", "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Bob'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(", "),n("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("dtype")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'object'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h2",{attrs:{id:"访问元素-2"}},[t._v("访问元素")]),t._v(" "),n("h3",{attrs:{id:"访问部分数据"}},[t._v("访问部分数据")]),t._v(" "),n("ul",[n("li",[t._v("通过关键字 "),n("code",[t._v("pd.DataFrame(index, columns)")]),t._v(" 访问部分数据")]),t._v(" "),n("li",[t._v("使用方法 "),n("code",[t._v("df.loc[:,'column_1':'column_2']")]),t._v(" 通过"),n("strong",[t._v("列标签")]),t._v("访问从 "),n("code",[t._v("column_1")]),t._v(" 到 "),n("code",[t._v("column_2")]),t._v(" 列的所有行")]),t._v(" "),n("li",[t._v("使用方法 "),n("code",[t._v("df.iloc[:,column_number1:column_number2]")]),t._v(" 通过"),n("strong",[t._v("列数字标签")]),t._v("访问从 "),n("code",[t._v("column_1")]),t._v(" 到 "),n("code",[t._v("column_number2")]),t._v(" 列的所有行")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n\nitems "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Bob'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Series"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("245")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("25")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("55")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bike'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'watch'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n         "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Alice'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Series"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("40")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("110")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("500")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'book'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'glasses'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bike'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\nshopping_carts "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nshopping_carts\n\nbob_shopping_cart "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" columns"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Bob'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nbob_shopping_cart\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("\tAlice\tBob\nbike\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("500.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("245.0")]),t._v("\nbook\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("40.0")]),t._v("\tNaN\nglasses\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("110.0")]),t._v("\tNaN\npants\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("25.0")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("watch")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("55.0")]),t._v("\n\n\tBob\nbike\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("245")]),t._v("\npants\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("25")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("watch")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("55")]),t._v("\n")])])]),n("p",[t._v("💡 使用关键字 "),n("code",[t._v("np.r_")]),t._v(" 访问不连续的列")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 访问 dataframe 不连续的列，即第0列和第1列，第23列至第32列")]),t._v("\ndf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("iloc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("np"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("r_"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("23")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("33")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),n("h3",{attrs:{id:"访问列"}},[t._v("访问列")]),t._v(" "),n("p",[t._v("使用列标签组成的"),n("strong",[t._v("列表")]),t._v("查询多个（单个）列数据 "),n("code",[t._v("df[['columns_name1', 'columns_name2']]")])]),t._v(" "),n("p",[t._v("同样可以使用点 "),n("code",[t._v(".")]),t._v(" 访问相应的列，"),n("code",[t._v("df.volumn_name")]),t._v(" 返回相应的列，类型为 Series，结果与前一种方法相同。")]),t._v(" "),n("h3",{attrs:{id:"访问行"}},[t._v("访问行")]),t._v(" "),n("p",[t._v("使用行标签组成的"),n("strong",[t._v("列表")]),t._v("，并用属性 "),n("code",[t._v(".loc")]),t._v(" 查询多（单）行数据 "),n("code",[t._v("df.loc[['index1', index2']]")])]),t._v(" "),n("h3",{attrs:{id:"访问元素-3"}},[t._v("访问元素")]),t._v(" "),n("p",[t._v("使用行标签和列标签查询对应元素的值 "),n("code",[t._v("df[column][row]")])]),t._v(" "),n("p",[t._v("⚠️ 标签顺序是先给出"),n("strong",[t._v("列标签")]),t._v("，再给出"),n("strong",[t._v("行标签")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# We print the store_items DataFrame")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("store_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# We access rows, columns and elements using labels")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'How many bikes are in each store:\\n'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" store_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bikes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'How many bikes and pants are in each store:\\n'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" store_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bikes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'What items are in Store 1:\\n'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" store_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'store 1'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'How many bikes are in Store 2:'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" store_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bikes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'store 2'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("\tbikes\tglasses\tpants\twatches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\n\nHow many bikes are "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" each store:\n\n\tbikes\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\n\nHow many bikes and pants are "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" each store:\n\n\tbikes\tpants\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\n\nWhat items are "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" Store "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(":\n\n\tbikes\tglasses\tpants\twatches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\n\nHow many bikes are "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" Store "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v(": "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\n")])])]),n("h3",{attrs:{id:"方法query"}},[t._v("方法query")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".query('condition_expr')")]),t._v(" 返回满足表达式 "),n("code",[t._v("econdition_expr")]),t._v("（即布尔值为 "),n("code",[t._v("True")]),t._v("）的数据集，其中 "),n("code",[t._v("condition_expr")]),t._v(" 是一个条件表达式，与"),n("strong",[t._v("特定列")]),t._v("相关的条件运算，返回一个布尔值")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用方法 query 筛选出 diagnosis 项的值等于 M 的数据项（行）")]),t._v("\ndf_m "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("query"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'diagnosis == \"M\"'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#实现相同功能的方法")]),t._v("\ndf_m "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'diagnosis'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'M'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),n("p",[t._v("⚠️")]),t._v(" "),n("ul",[n("li",[t._v("条件表达式需要使用"),n("strong",[t._v("单引号 "),n("code",[t._v("''")]),t._v(" 括起来")])]),t._v(" "),n("li",[t._v("条件语句中的字符串则用双引号 "),n("code",[t._v('""')]),t._v(" 括起来，但"),n("strong",[t._v("列标签")]),t._v("不需要，可（类似变量）直接使用")]),t._v(" "),n("li",[t._v("可使用关键词构建"),n("strong",[t._v("复杂的条件表达式")]),t._v("：\n"),n("ul",[n("li",[n("code",[t._v("&")]),t._v(" 和 "),n("code",[t._v("and")]),t._v(" 表示条件"),n("strong",[t._v("和")])]),t._v(" "),n("li",[n("code",[t._v("|")]),t._v(" 和 "),n("code",[t._v("or")]),t._v(" 表示条件"),n("strong",[t._v("或")])])])])]),t._v(" "),n("h2",{attrs:{id:"添加元素"}},[t._v("添加元素")]),t._v(" "),n("h3",{attrs:{id:"添加列"}},[t._v("添加列")]),t._v(" "),n("ul",[n("li",[t._v("类似于字典添加新元素的方式，通过 "),n("code",[t._v("df[new_column_name] = [value1, value2, value3]")]),t._v("（假设 "),n("code",[t._v("df")]),t._v(" 含有三行数据）")]),t._v(" "),n("li",[t._v("默认将新列添加到了 DataFrame 的"),n("strong",[t._v("末尾")])])]),t._v(" "),n("h3",{attrs:{id:"添加列到特定的行"}},[t._v("添加列到特定的行")]),t._v(" "),n("p",[t._v("仅向 DataFrame 中"),n("strong",[t._v("特定行")]),t._v("添加新的列，其他行使用 "),n("code",[t._v("NaN")]),t._v(" 填充。")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".insert(loc, label, data)")]),t._v(" 将列插入到"),n("strong",[t._v("指定的位置")]),t._v("，关键字 "),n("code",[t._v("loc")]),t._v(" 指定新列插入到哪个"),n("strong",[t._v("索引前")]),t._v("，并以 "),n("code",[t._v("label")]),t._v(" 作为列标签")]),t._v(" "),n("h3",{attrs:{id:"添加行"}},[t._v("添加行")]),t._v(" "),n("p",[t._v("需要使用方法 "),n("code",[t._v(".append()")]),t._v(" 将新的 "),n("strong",[t._v("DataFrame")]),t._v(" 添加到原始数据框的末尾，若添加的新 DataFrame 引入新的列后，所有"),n("strong",[t._v("列会按照字母顺序排序")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 打印原始的数据框")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("store_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 添加列")]),t._v("\nstore_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shirts'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nstore_items\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 添加行")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 先创建新的 DataFrame")]),t._v("\nnew_items "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bikes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'watches'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'glasses'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nnew_store "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("new_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'store 3'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nnew_store\nstore_items "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" store_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("new_store"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nstore_items\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用运算（直接截取/指定） 部分数据产生新的列")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 只往特定行添加新的列")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在商店 2 和 3 中上一批新手表，并且新手表的数量与这些商店原有手表的库存一样")]),t._v("\nstore_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'new watches'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" store_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'watches'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nstore_items\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 特定位置插入新列，使用方法 .insert(loc, label, data) 指定插入的位置")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将名称为 shoes 的新列插入 suits 列前面（因为 suits 的数字索引值为 4）")]),t._v("\nstore_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("insert"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shoes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nstore_items\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("\tbikes\tglasses\tpants\twatches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\n\n\tbikes\tglasses\tpants\twatches\tshirts\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\n\n\tbikes\tglasses\tpants\twatches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\n\n\tbikes\tglasses\tpants\tshirts\tsuits\twatches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\tNaN\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\n\n\tbikes\tglasses\tpants\tshirts\tsuits\twatches\tnew watches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\tNaN\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10.0")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\tNaN\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35.0")]),t._v("\n\n\tbikes\tglasses\tpants\tshirts\tshoes\tsuits\twatches\tnew watches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\tNaN\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10.0")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35.0")]),t._v("\n")])])]),n("h2",{attrs:{id:"删除元素-2"}},[t._v("删除元素")]),t._v(" "),n("h3",{attrs:{id:"方法pop"}},[t._v("方法pop")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".pop()")]),t._v(" 允许我们"),n("strong",[t._v("删除列")]),t._v("，一般默认删除"),n("strong",[t._v("最后一列")])]),t._v(" "),n("h3",{attrs:{id:"方法drop"}},[t._v("方法drop")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".drop()")]),t._v(" 可以同时用于删除行和列，使用关键字 "),n("code",[t._v("axis")]),t._v(" 指定要删除的维度，"),n("code",[t._v("axis = 0")]),t._v(" 指行，"),n("code",[t._v("axis = 1")]),t._v(" 指列")]),t._v(" "),n("h3",{attrs:{id:"删除重复项-行"}},[t._v("删除重复项（行）")]),t._v(" "),n("ul",[n("li",[t._v("使用方法 "),n("code",[t._v(".drop_duplicates(subset=None, keep='first', inplace=False)")]),t._v(" 删除重复项")]),t._v(" "),n("li",[t._v("使用方法 "),n("code",[t._v(".drop_duplication(subset_column)")]),t._v(" 根据指定的列 "),n("code",[t._v("subset_column")]),t._v(" 重复情况删除相应的项")]),t._v(" "),n("li",[t._v("删除默认输出"),n("strong",[t._v("副本")]),t._v("，可以将关键字设置为 "),n("code",[t._v("inplace = True")]),t._v(" 在原数据上修改")]),t._v(" "),n("li",[t._v("关键字 "),n("code",[t._v("keep='first'")]),t._v(" 默认保留第一次出现的项，删除其余的重复项，可更改为 "),n("code",[t._v("keep='last'")]),t._v(" 将保留项选为最后出现的重复项")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("store_items\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 删除最后一列 new watches")]),t._v("\nstore_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'new watches'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nstore_items\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 删除指定列，轴设置为 axis = 1，列标签为 watches 和 shoes")]),t._v("\nstore_items "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" store_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'watches'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shoes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inplace "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nstore_items\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 删除指定行，轴设置为 axis = 0，行标签为 store 2 和 store 1")]),t._v("\nstore_items "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" store_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'store 2'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'store 1'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inplace "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nstore_items\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在原始的 df 删除重复项")]),t._v("\ndf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop_duplicates"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("inplace"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 检查 df 中是否还有项")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'The duplicated entries is: '")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("duplicated"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("\tbikes\tglasses\tpants\tshirts\tshoes\tsuits\twatches\tnew watches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\tNaN\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10.0")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35.0")]),t._v("\n\n\tbikes\tglasses\tpants\tshirts\tshoes\tsuits\twatches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\n\n\tbikes\tglasses\tpants\tshirts\tsuits\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45.0")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7.0")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\tNaN\tNaN\n\n\tbikes\tglasses\tpants\tshirts\tsuits\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\tNaN\tNaN\n\nThe duplicated entries is: "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n")])])]),n("h2",{attrs:{id:"更改标签"}},[t._v("更改标签")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".rename()")]),t._v(" 更改行和列标签，列标签关键词为 "),n("code",[t._v("columns")]),t._v("，行标签关键词为 "),n("code",[t._v("index")])]),t._v(" "),n("p",[t._v("⚠️ 该修改默认"),n("strong",[t._v("不在原数据集")]),t._v("上修改，可设置参数 "),n("code",[t._v("inplace = True")]),t._v(" 直接在原数据上修改")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("store_items\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将列标签 bikes 改为 hats")]),t._v("\nstore_items "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" store_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rename"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("columns "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bikes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'hats'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inplace "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将行标签 store 3 改为 last store")]),t._v("\nstore_items "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" store_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rename"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'store 3'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'last store'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inplace "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nstore_items\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("\tbikes\tglasses\tpants\tshirts\tsuits\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\tNaN\tNaN\n\n\t        hats\tglasses\tpants\tshirts\tsuits\nlast store\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\tNaN\tNaN\n")])])]),n("h3",{attrs:{id:"将行索引改为单独列"}},[t._v("将行索引改为单独列")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".set_index(new_columns_name)")]),t._v(" 为"),n("strong",[t._v("行索引")]),t._v("设置"),n("strong",[t._v("列标签")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("store_items\n\nstore_items "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" store_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("set_index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nstore_items\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("\t        hats\tglasses\tpants\tshirts\tsuits\nlast store\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\tNaN\tNaN\n\npants\thats\tglasses\tshirts\tsuits\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),t._v("\tNaN\tNaN\n")])])]),n("h2",{attrs:{id:"处理元素-清理数据"}},[t._v("处理元素/清理数据")]),t._v(" "),n("ul",[n("li",[t._v("离群值")]),t._v(" "),n("li",[t._v("不正确的值")]),t._v(" "),n("li",[t._v("缺少值")])]),t._v(" "),n("h3",{attrs:{id:"缺少值"}},[t._v("缺少值")]),t._v(" "),n("p",[t._v("Pandas 会为缺少的值的元素分配 "),n("code",[t._v("NaN")]),t._v(" 值,使用大型数据集训练学习算法之前需要"),n("strong",[t._v("检测和处理 "),n("code",[t._v("NaN")]),t._v(" 值")])]),t._v(" "),n("h4",{attrs:{id:"计算-nan-数量"}},[t._v("计算 "),n("code",[t._v("NaN")]),t._v(" 数量")]),t._v(" "),n("h5",{attrs:{id:"计算-nan-总个数"}},[t._v("计算 "),n("code",[t._v("NaN")]),t._v(" 总个数")]),t._v(" "),n("ul",[n("li",[t._v("使用方法 "),n("code",[t._v(".isnull()")]),t._v(" 返回一个大小和 store_items 一样的"),n("strong",[t._v("布尔型 DataFrame")])]),t._v(" "),n("li",[t._v("用 "),n("code",[t._v("True")]),t._v(" 表示具有 "),n("code",[t._v("NaN")]),t._v(" 值的元素，用 "),n("code",[t._v("False")]),t._v(" 表示非 "),n("code",[t._v("NaN")]),t._v(" 值的元素")]),t._v(" "),n("li",[t._v("在 Pandas 中，逻辑值 "),n("code",[t._v("True")]),t._v(" 的数字值是 "),n("code",[t._v("1")]),t._v("，逻辑值 "),n("code",[t._v("False")]),t._v(" 的数字值是 "),n("code",[t._v("0")]),t._v("，可以"),n("strong",[t._v("通过数逻辑值 "),n("code",[t._v("True")]),t._v(" 的数量数出 "),n("code",[t._v("NaN")]),t._v(" 值的数量")])]),t._v(" "),n("li",[t._v("结合方法 "),n("code",[t._v(".sum()")]),t._v(" 可以统计 "),n("code",[t._v("NaN")]),t._v(" 的数量\n"),n("ul",[n("li",[t._v("使用一次方法 "),n("code",[t._v(".sum()")]),t._v(" 返回一个 Pandas Series，存储了"),n("strong",[t._v("列上的逻辑值 "),n("code",[t._v("True")]),t._v(" 的数量")]),t._v("（即 "),n("code",[t._v("NaN")]),t._v(" 数量）")]),t._v(" "),n("li",[t._v("再使用一次方法 "),n("code",[t._v(".sum()")]),t._v(" 返回 DataFrame 总的 "),n("code",[t._v("NaN")]),t._v(" 数量")])])])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("items2 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bikes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'watches'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shirts'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shoes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'suits'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'watches'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'glasses'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bikes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shirts'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shoes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'suits'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bikes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'watches'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'glasses'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shoes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\nstore_items "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("items2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'store 1'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'store 2'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'store 3'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nstore_items\n\nstore_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("isnull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nstore_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("isnull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nx "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("  store_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("isnull"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Number of NaN values in our DataFrame:'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("\tbikes\tglasses\tpants\tshirts\tshoes\tsuits\twatches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\n\n\tbikes\tglasses\tpants\tshirts\tshoes\tsuits\twatches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\tFalse\tTrue\tFalse\tFalse\tFalse\tFalse\tFalse\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\tFalse\tFalse\tFalse\tTrue\tFalse\tTrue\tFalse\n\nbikes            "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\nglasses        "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\npants           "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\nshirts           "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\nshoes          "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\nsuits            "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\nwatches      "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\ndtype: int64\n\nNumber of NaN values "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" our DataFrame: "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\n")])])]),n("h5",{attrs:{id:"分维度统计-nan-数量"}},[t._v("分维度统计 "),n("code",[t._v("NaN")]),t._v(" 数量")]),t._v(" "),n("p",[t._v("使用函数 "),n("code",[t._v("df.count()")]),t._v(" 返回"),n("strong",[t._v("非缺失值")]),t._v("个数（默认按列），设置"),n("strong",[t._v("参数 "),n("code",[t._v("axis")]),t._v(" 如 "),n("code",[t._v("axis=1")]),t._v(" 按照行来数非缺失值的个数")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("df\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 每列缺失值的个数")]),t._v("\ndf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("count"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 每行缺失值的个数")]),t._v("\ndf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("count"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("axis"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("       a           c          b\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("      "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("          "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("          NaN\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("      "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("          "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("          "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("      "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("         NaN      NaN\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("      "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("         NaN      "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n\na    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\nb    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\nc    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("      "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("      "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("      "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("      "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n")])])]),n("h5",{attrs:{id:"方法-count"}},[t._v("方法 "),n("code",[t._v("count")])]),t._v(" "),n("p",[t._v("直接使用方法 "),n("code",[t._v(".count")]),t._v(" 统计"),n("strong",[t._v("非 "),n("code",[t._v("NaN")]),t._v(" 值")]),t._v("得数量")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("items2 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bikes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'watches'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shirts'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shoes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'suits'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'watches'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'glasses'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bikes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shirts'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shoes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'suits'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bikes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'watches'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'glasses'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shoes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\nstore_items "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("items2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'store 1'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'store 2'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'store 3'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nstore_items\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Number of non-NaN values in the columns of our DataFrame:\\n'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" store_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("count"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("\tbikes\tglasses\tpants\tshirts\tshoes\tsuits\twatches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\n\nNumber of non-NaN values "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" the columns of our DataFrame:\nbikes            "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\nglasses        "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\npants           "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\nshirts           "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\nshoes          "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\nsuits            "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\nwatches      "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\ndtype: int64\n")])])]),n("h4",{attrs:{id:"处理-nan"}},[t._v("处理 "),n("code",[t._v("NaN")])]),t._v(" "),n("h5",{attrs:{id:"删除-nan"}},[t._v("删除 "),n("code",[t._v("NaN")])]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".dropna(axis)")]),t._v(" 删除包含 "),n("code",[t._v("NaN")]),t._v(" 值得任何行或列，其中 "),n("code",[t._v("axis = 0")]),t._v(" 指行，"),n("code",[t._v("axis = 1")]),t._v(" 指列。修改参数 "),n("code",[t._v("subset=['column1', 'column2']")]),t._v(" 指定删除"),n("strong",[t._v("特定列中含有 "),n("code",[t._v("NaN")]),t._v(" 值")]),t._v("的行。")]),t._v(" "),n("p",[t._v("⚠️ 方法 "),n("code",[t._v(".dropna()")]),t._v(" 中要将"),n("strong",[t._v("关键字设置为 "),n("code",[t._v("inplace=True")])]),t._v("，才可在原始 df 中删除目标行或列。")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("items2 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bikes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'watches'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shirts'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shoes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'suits'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'watches'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'glasses'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bikes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shirts'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shoes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'suits'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bikes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pants'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'watches'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'glasses'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'shoes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\nstore_items "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("items2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" index "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'store 1'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'store 2'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'store 3'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nstore_items\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# We drop any rows with NaN values")]),t._v("\nstore_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dropna"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("axis "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inplace "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# We drop any columns with NaN values")]),t._v("\nstore_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dropna"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("axis "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inplace "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("\tbikes\tglasses\tpants\tshirts\tshoes\tsuits\twatches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\n\n\tbikes\tglasses\tpants\tshirts\tshoes\tsuits\twatches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\n\n\tbikes\tpants\tshoes\twatches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\n")])])]),n("h5",{attrs:{id:"填充nan"}},[t._v("填充NaN")]),t._v(" "),n("p",[t._v("使用"),n("strong",[t._v("方法 "),n("code",[t._v(".fillna()")])]),t._v(" 填充 "),n("code",[t._v("NaN")]),t._v("，可选择多种填充方式；或使用"),n("strong",[t._v("方法 "),n("code",[t._v(".interpolate()")])]),t._v(" 填充 "),n("code",[t._v("NaN")]),t._v("，可选择不同的"),n("strong",[t._v("插值方法")])]),t._v(" "),n("ul",[n("li",[n("p",[t._v("方法 "),n("code",[t._v(".fillna()")]),t._v(" 填充"),n("strong",[t._v("特定值")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用 0 填充 NaN")]),t._v("\nstore_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fillna"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nstore_items\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("\tbikes\tglasses\tpants\tshirts\tshoes\tsuits\twatches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\n")])])])]),t._v(" "),n("li",[n("p",[t._v("方法 "),n("code",[t._v(".fillna()")]),t._v(" 前向填充\n将方法关键字 "),n("code",[t._v("method")]),t._v(" 设置为 "),n("code",[t._v("'ffill'")]),t._v("，会沿着给定 "),n("code",[t._v("axis")]),t._v(" 使用上个已知值替换 "),n("code",[t._v("NaN")]),t._v("值，当 "),n("code",[t._v("axis = 0")]),t._v(" 时"),n("strong",[t._v("即以上一行")]),t._v("对应位置的值填充；当 "),n("code",[t._v("axis = 1")]),t._v(" 时以"),n("strong",[t._v("即以前一列")]),t._v("对应位置的值填充")])]),t._v(" "),n("li",[n("p",[t._v("方法 "),n("code",[t._v(".fillna()")]),t._v(" 后向填充\n将方法关键字 "),n("code",[t._v("method")]),t._v(" 设置为 "),n("code",[t._v("'backfill'")]),t._v("，沿着给定 "),n("code",[t._v("axis")]),t._v(" 使用下个已知值替换 "),n("code",[t._v("NaN")]),t._v(" 值")])]),t._v(" "),n("li",[n("p",[t._v("常使用列的"),n("strong",[t._v("平均值")]),t._v("填充相应列的 "),n("code",[t._v("NaN")]),t._v("\n使用 "),n("code",[t._v("df.fillna(df.mean(), inplace=True)")]),t._v(" 实现均值填充")])]),t._v(" "),n("li",[n("p",[t._v("方法 "),n("code",[t._v(".interpolate()")]),t._v(" 的 "),n("code",[t._v("linear")]),t._v(" 函数插值\n使用 "),n("code",[t._v(".interpolate(method = 'linear', axis)")]),t._v(" 方法，利用 "),n("code",[t._v("linear")]),t._v(" 函数计算插值")])])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 使用 linear 函数进行 interpolation using column values")]),t._v("\nstore_items"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("interpolate"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("method "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'linear'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("\tbikes\tglasses\tpants\tshirts\tshoes\tsuits\twatches\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\tNaN\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\nstore "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("35")]),t._v("\n")])])]),n("p",[n("code",[t._v("store 3")]),t._v(" 中的两个 "),n("code",[t._v("NaN")]),t._v(" 值被替换成了线性插值。但是注意 "),n("code",[t._v("store 1")]),t._v(" 中的 "),n("code",[t._v("NaN")]),t._v(" 值没有被替换掉。因为该 "),n("code",[t._v("NaN")]),t._v(" 值是该列中的第一个值（即"),n("strong",[t._v("没有前一行")]),t._v("），因此"),n("strong",[t._v("插值函数无法计算值")]),t._v("，可改用 "),n("code",[t._v("axis = 1")]),t._v(" 根据前一列相应位置计算插值。")]),t._v(" "),n("p",[t._v("⚠️ 同样地 "),n("code",[t._v(".interpolate()")]),t._v(" 方法不在原地替换 "),n("code",[t._v("NaN")]),t._v(" 值，将"),n("strong",[t._v("关键字 "),n("code",[t._v("inplace")]),t._v(" 设为 "),n("code",[t._v("True")])])]),t._v(" "),n("h3",{attrs:{id:"重复行"}},[t._v("重复行")]),t._v(" "),n("h4",{attrs:{id:"查看重复行"}},[t._v("查看重复行")]),t._v(" "),n("ul",[n("li",[t._v("使用方法 "),n("code",[t._v("df.duplicated()")]),t._v(" 返回一个 pandas Series 使用布尔值 "),n("code",[t._v("False")]),t._v(" 或 "),n("code",[t._v("True")]),t._v(" 来表示对应行是否味重复行。")]),t._v(" "),n("li",[t._v("使用函数 "),n("code",[t._v("sum(df.duplicated())")]),t._v(" 统计 dataframe 中重复值")])]),t._v(" "),n("h4",{attrs:{id:"删除重复行"}},[t._v("删除重复行")]),t._v(" "),n("ul",[n("li",[t._v("使用方法 "),n("code",[t._v(".drop_duplicates(subset=None, keep='first', inplace=False)")]),t._v(" 删除重复项")]),t._v(" "),n("li",[t._v("默认以"),n("strong",[t._v("所有列的值相同")]),t._v("为重复行评判标准，可以修改"),n("strong",[t._v("关键词 "),n("code",[t._v("subset=column_name")])]),t._v(" 根据指定的列 "),n("code",[t._v("subset_column")]),t._v(" 重复情况寻找特殊的重复行")]),t._v(" "),n("li",[t._v("删除默认输出"),n("strong",[t._v("副本")]),t._v("，可以将关键字设置为 "),n("code",[t._v("inplace = True")]),t._v("，直接在原数据上修改")]),t._v(" "),n("li",[t._v("关键字 "),n("code",[t._v("keep='first'")]),t._v(" 默认保留第一次出现的项，删除其余的重复项，可更改为"),n("code",[t._v("keep='last'")]),t._v(" 将保留项选为最后出现的重复项")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 在原始的 df 删除重复项")]),t._v("\ndf"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop_duplicates"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("inplace"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 检查 df 中是否还有项")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'The duplicated entries is: '")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("duplicated"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h3",{attrs:{id:"数据提取"}},[t._v("数据提取")]),t._v(" "),n("h4",{attrs:{id:"从字符串中提取整型"}},[t._v("从字符串中提取整型")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".str.extract('(\\d+)').astype(int)")]),t._v(" 其中 "),n("code",[t._v("\\d+")]),t._v(" 是正则表达式，用以匹配"),n("strong",[t._v("数字")])]),t._v(" "),n("h4",{attrs:{id:"分裂数据"}},[t._v("分裂数据")]),t._v(" "),n("h5",{attrs:{id:"分裂为行"}},[t._v("分裂为行")]),t._v(" "),n("p",[t._v("结合"),n("a",{attrs:{href:"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("函数"),n("OutboundLink")],1),t._v(" "),n("code",[t._v(".apply")]),t._v(" 和 Lambda 表达式创建一个函数，沿着特定 "),n("code",[t._v("axis")]),t._v(" 方向对每个元素进行操作并用运算"),n("strong",[t._v("返回的值取代相应元素")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对使用分隔符 / 包含多个值的单元格数据进行拆分为相应的多行，数据类型为 str")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 提取数据框中特定列 fuel 中包含分隔符 / 对应的行")]),t._v("\ndf_multi "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'fuel'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("contains"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\ndf_multi\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("                    model  displ        cyl      trans  drive           fuel    veh_class    air_pollution_score  city_mpg    hwy_mpg    cmb_mpg  greenhouse_gas_score  smartway\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("582")]),t._v("    MERCEDES-BENZ C300    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.0")]),t._v("    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v(" cyl"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("    Auto-L7    2WD    ethanol/gas    small car                    "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v("/4     "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("13")]),t._v("/18      "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("19")]),t._v("/25      "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("/21                   "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),t._v("/6        no\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 查询获得只有一项数据的 fuel 列含有多个值")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 由于单个单元格包含的复合值数量是两个，需要创建相应的数据项两个副本")]),t._v("\ndf1 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df_multi"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("copy"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 每个混合动力车第一种燃料类型的数据")]),t._v("\ndf2 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df_multi"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("copy"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 每个混合动力车第二种燃料类型的数据")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 可以找到除了目标列 fuel 含有多个值以外，另外几个特征也使用分隔符 / 包含多个值，在分隔时需要统一处理，使用列表 split_columns 收集这些列标签")]),t._v("\nsplit_columns "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'fuel'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'air_pollution_score'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'city_mpg'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'hwy_mpg'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cmb_mpg'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'greenhouse_gas_score'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 对每个数据框副本的每个列应用分割功能")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" c "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" split_columns"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    df1"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("c"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df1"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("c"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("apply")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 提取分割获得的第一个值替换复合值")]),t._v("\n    df2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("c"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("c"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("apply")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 提取分割获得的第二个值替换复合值")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 合并两个数据框")]),t._v("\nnew_rows "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df1"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 丢弃原始数据行")]),t._v("\ndf_08"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("drop"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df_multi"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inplace"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 添加新分割的行，注意添加时需要忽略行标签（构建的数据框 new_rows 两行的标签都是 582），将新添加的行加到末尾")]),t._v("\ndf "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("new_rows"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ignore_index"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndf_08"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tail"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("🔨")]),t._v(" "),n("div",{staticClass:"language-shell extra-class"},[n("pre",{pre:!0,attrs:{class:"language-shell"}},[n("code",[t._v("\tmodel\tdispl\tcyl\ttrans\tdrive\tfuel\tveh_class\tair_pollution_score\tcity_mpg\thwy_mpg\tcmb_mpg\tgreenhouse_gas_score\tsmartway\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("982")]),t._v("\tVOLVO XC "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("90")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v(" cyl"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\tAuto-S6\t2WD\tGasoline\tSUV\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("14")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("\tno\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("983")]),t._v("\tVOLVO XC "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("90")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.2")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v(" cyl"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\tAuto-S6\t4WD\tGasoline\tSUV\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("14")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("\tno\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("984")]),t._v("\tVOLVO XC "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("90")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.4")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),t._v(" cyl"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\tAuto-S6\t4WD\tGasoline\tSUV\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("13")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("19")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\tno\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("985")]),t._v("\tMERCEDES-BENZ C300\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v(" cyl"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\tAuto-L7\t2WD\tethanol\tsmall car\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("13")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("19")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("15")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),t._v("\tno\n"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("986")]),t._v("\tMERCEDES-BENZ C300\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.0")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v(" cyl"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\tAuto-L7\t2WD\tgas\tsmall car\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("18")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("25")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("21")]),t._v("\t"),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v("\tno\n")])])]),n("h5",{attrs:{id:"分裂为列"}},[t._v("分裂为列")]),t._v(" "),n("h3",{attrs:{id:"数据类型转换"}},[t._v("数据类型转换")]),t._v(" "),n("h4",{attrs:{id:"将浮点型转换为整数型"}},[t._v("将浮点型转换为整数型")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".astype(int)")]),t._v(" 将浮点数据转换为"),n("strong",[t._v("整数型")])]),t._v(" "),n("h2",{attrs:{id:"数据可视化"}},[t._v("数据可视化")]),t._v(" "),n("p",[t._v("⚠️ 若使用 Jupyter notebook 进行可视化操作并显示，需要在代码框输入 "),n("code",[t._v("% matplotlib inline")])]),t._v(" "),n("h3",{attrs:{id:"通用绘图"}},[t._v("通用绘图")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".plot（kind='plot_method')")]),t._v(" 过关键字 "),n("code",[t._v("kind")]),t._v(" 设置图形类型，如")]),t._v(" "),n("ul",[n("li",[t._v("直方图 "),n("code",[t._v("hist")])]),t._v(" "),n("li",[t._v("柱形图 "),n("code",[t._v("bar")])]),t._v(" "),n("li",[t._v("饼图 "),n("code",[t._v("pie")])]),t._v(" "),n("li",[t._v("箱线图 "),n("code",[t._v("box")])])]),t._v(" "),n("p",[t._v("💡 绘制基于"),n("strong",[t._v("分类统计")]),t._v("的图形时（如柱状图，饼图等），可使用方法 "),n("code",[t._v("value_counts()")]),t._v(" 来快速"),n("strong",[t._v("统计每种值得数量")])]),t._v(" "),n("h3",{attrs:{id:"直方图"}},[t._v("直方图")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".hist()")]),t._v(" 绘制直方图，")]),t._v(" "),n("h3",{attrs:{id:"散点图"}},[t._v("散点图")]),t._v(" "),n("h4",{attrs:{id:"单个散点图"}},[t._v("单个散点图")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".plot.scatter(x='column_1', y='column_2')")])]),t._v(" "),n("h4",{attrs:{id:"散点图矩阵"}},[t._v("散点图矩阵")]),t._v(" "),n("p",[t._v("使用函数 "),n("code",[t._v("pd.scatter_matrix(dataframe)")]),t._v(" 绘制多变量间（各列属性）散点图阵列，快速了解变量间得"),n("strong",[t._v("相关关系")])]),t._v(" "),n("h3",{attrs:{id:"调整画布大小"}},[t._v("调整画布大小")]),t._v(" "),n("p",[t._v("使用关键字 "),n("code",[t._v("figsize=(x, y)")]),t._v(" 将画布设置成 "),n("code",[t._v("x*y")]),t._v(" 的大小")]),t._v(" "),n("h2",{attrs:{id:"导出dataframe"}},[t._v("导出DataFrame")]),t._v(" "),n("p",[t._v("将数据框导出")]),t._v(" "),n("h3",{attrs:{id:"写入csv文件"}},[t._v("写入CSV文件")]),t._v(" "),n("p",[t._v("使用方法 "),n("code",[t._v(".to_csv('file_name')")]),t._v(" 将 dataframe 保存到 "),n("code",[t._v("csv")]),t._v(" 文件中")]),t._v(" "),n("p",[t._v("⚠️ 默认保存"),n("strong",[t._v("索引")]),t._v("，若需要忽略（不保存索引"),n("strong",[t._v("参数修改为 "),n("code",[t._v("index=False")])]),t._v("）")])])}),[],!1,null,null,null);s.default=e.exports},607:function(t,s,a){t.exports=a.p+"assets/img/20181208213302672_27095.1e9a07c6.png"}}]);